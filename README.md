# Capstone2_Kaggle_Project
MSBA Capstone2 Kaggle Group Project

# Business Problem to solve: The purpose of this project is to submit the final results for the Home Credit Default risk Kaggle  project, with the end goal of creating an accurate model to classify whether or not an individual will repay a loan. One of the key components that generally enables an individual to receive a loan is credit history; however, not all consumers have a sufficient credit history to be eligible for loans, even if they would otherwise be considered worthy recipients. Additionally, the datasets available to analyze the worthiness of an applicant can be messy and require various data transformations. As such, it is our task to take into consideration other alternative factors to determine the creditworthiness of individuals applying for a home loan, transform the data appropriately, and create a satisfactory model. 

# My group's solution: My group leveraged various analytic and machine learning approaches to finalize a high performance model. Prior to this, we individually conducted exploratory data analysis, data preparation, and model submission. Greg started the process off by helping with some in depth EDA, data transformation, and problem wire framing, then We submitted modeling in Kaggle using Hunter's logistic regression model (0.59), Tom's h2o and lime model to do a matrix statistic analysis (0.68), and Hari's XGBoost modeling (0.7). The XGBoost resulted in the highest Kaggle score (0.7), and is therefore the model we settled on leveraging. 

# My contribution: I created a logistic model, which though it had the lowest Kaggle score, was the method that was most explainable to stakeholders. I also played a key role in EDA, coordinating contributions, maintaining communication, and compiling notebooks to be ready for final submission. 

# Business Value: We have created a machine learning method of determining with 92% accuracy and a Kaggle score of .78 the likelihood that a loan recipient would default on their mortgage. While slightly more computationally expensive than other models, the tradeoff in performance seems to be absolutely worth it. The avoidance of dedicating resources to an extremely risky recipient enables Home Credit to continue functioning as a successful business model, and continue to service their community. 

# Difficulties: There was definitely a cultural and communications barriers within our group. That is typical in a business setting though, and managing different personalities and capabilities is key to ensuring you're able to deliver results. We also had problems framing the question as a business problem rather than an analytical problem. I think our presentation itself also suffered from being underprepared, which could have been resolved with more commitment from myself and other members.

# What I learned: Frankly, the most valuable thing I believe I learned from this project were models such as H20, Lime, and XGBoost. While they are models that I learned about online after seeing my fellow students implement them, I didn't know they were available to be used prior to this project. I learned that even if you have an amazing analytical solution to a problem, it ultimately isn't going to be useful until you can present it in such a way to get buy in to both your methodology and results.  
